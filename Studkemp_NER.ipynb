{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a test version of NER module for project for extracting important info from text. This version works on english dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wLG4H0xY7j_"
      },
      "source": [
        "# Installing spacy core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pFuSCbJXew7P",
        "outputId": "637120d1-2abf-466c-fce1-95877291198d"
      },
      "outputs": [],
      "source": [
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthR8gn5glwH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtmVbYYpWmET",
        "outputId": "d31e4f86-7846-4eb1-9bda-76b2f3e57239"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import spacy\n",
        "print(spacy.__version__)\n",
        "import re\n",
        "import random\n",
        "\n",
        "spacy.prefer_gpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esKH5piGZsiI"
      },
      "source": [
        "# Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTFwmLKQaHvX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "acd_JqUsZsxH",
        "outputId": "ccd1ec3d-5431-433d-e35d-4fa5446fc992"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = \"https://www.kaggle.com/datasets/harsh907/resume-entities-for-ner-2\"\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPA8lRMzceWH"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q7w70b6cdsP"
      },
      "outputs": [],
      "source": [
        "def trim_entity_spans(data: list) -> list:\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for ent in entities:\n",
        "            if len(ent) > 0 and len(ent[2]) > 0:\n",
        "                label = ent[2]\n",
        "                valid_start = ent[0]\n",
        "                valid_end = ent[1]\n",
        "                while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                        text[valid_start]):\n",
        "                    valid_start += 1\n",
        "                while valid_end > 1 and invalid_span_tokens.match(\n",
        "                        text[valid_end]):\n",
        "                    valid_end -= 1\n",
        "                if valid_start > valid_end:\n",
        "                    valid_start, valid_end = valid_end, valid_start\n",
        "                if valid_end != valid_start:\n",
        "                    valid_entities.append([valid_start, valid_end + 1, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "def validate_overlap(ALL_DATA):\n",
        "    for ix,x in enumerate(ALL_DATA):\n",
        "        startCK=[]\n",
        "        for iy,y in enumerate(x[-1]['entities']):\n",
        "            if iy == 0:\n",
        "                startCK.append([y[0],y[1]])\n",
        "            else:\n",
        "                pop = False\n",
        "                for z in startCK:\n",
        "                    if z[0] <= y[0] < z[1]:\n",
        "                        ALL_DATA[ix][-1]['entities'].pop(iy)\n",
        "                        pop = True\n",
        "                        break\n",
        "                if pop == False:\n",
        "                    startCK.append([y[0],y[1]])\n",
        "    return ALL_DATA\n",
        "\n",
        "def convert_doccano_to_spacy(doccano_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        with open(doccano_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = data.get('annotation', [])\n",
        "            spacy_ents = []\n",
        "            if entities:\n",
        "                for ent in entities:\n",
        "                    if ent and ent[\"label\"]:\n",
        "                        points = ent[\"points\"][0]\n",
        "                        start = points[\"start\"]\n",
        "                        end = points[\"end\"]\n",
        "                        label = ent[\"label\"][0]  # Assuming one label per entity\n",
        "                        spacy_ents.append((start, end, label))\n",
        "                training_data.append((text, {\"entities\": spacy_ents}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + doccano_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQP19EOTjEBv",
        "outputId": "2f4675c4-78d7-4619-a42e-dee8d785c85b"
      },
      "outputs": [],
      "source": [
        "DATA_FILE_PATH = \"./resume-entities-for-ner-2/train_Data1.json\"\n",
        "ALL_DATA = convert_doccano_to_spacy(DATA_FILE_PATH)\n",
        "ALL_DATA = trim_entity_spans(ALL_DATA)\n",
        "ALL_DATA = validate_overlap(ALL_DATA)\n",
        "random.shuffle(ALL_DATA)\n",
        "print(len(ALL_DATA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDRE8b2YtUH"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_83sB5go2w",
        "outputId": "e00da9b2-c6e2-46fc-b489-ab2da87907fc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "db = DocBin() # create a DocBin object\n",
        "\n",
        "c = 0\n",
        "for text, annot in tqdm(ALL_DATA): # data in previous format\n",
        "    doc = nlp.make_doc(text) # create doc object from text\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
        "        if span is None:\n",
        "            s = doc.text\n",
        "            sub_E = s[end:]\n",
        "            sub_S = s[:start]\n",
        "            end = end+ (0 if len(sub_E.split(\" \", 1)[0]) <= 0 else len(sub_E.split(\" \", 1)[0]))\n",
        "            start = start - (0 if len(sub_S.rsplit(\" \", 1)[-1]) <= 0 else len(sub_S.rsplit(\" \", 1)[-1]))\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span is None:\n",
        "                c+=1\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    pat_orig = len(ents)\n",
        "    filtered = filter_spans(ents) # THIS DOES THE TRICK\n",
        "    pat_filt =len(filtered)\n",
        "    doc.ents = filtered\n",
        "    db.add(doc)\n",
        "\n",
        "db.to_disk(\"./train.spacy\") # save the docbin object\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpfIm0gdDU0I"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey-cGyvTKGK6",
        "outputId": "a00a0dd4-361f-4904-c941-7b4c40770d09"
      },
      "outputs": [],
      "source": [
        "!python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency -F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yKHhWosH1n4",
        "outputId": "32d78954-7f5c-4913-86ab-55df6660b202"
      },
      "outputs": [],
      "source": [
        "! python -m spacy train config.cfg --output ./output_ --paths.train ./train.spacy --paths.dev ./train.spacy  --gpu-id 0 --training.max_epochs 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DNAF4NEmICgw",
        "outputId": "1f35e0fd-ac7d-4f5f-b73b-5e2fc43a24fe"
      },
      "outputs": [],
      "source": [
        "!mkdir base_model\n",
        "!python -m spacy package output_/model-best/ base_model/\n",
        "!pip install base_model/en_pipeline-0.0.0/dist/en_pipeline-0.0.0.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQwjBUYtILaw"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h2KgsYXUINUY",
        "outputId": "31fa9eb9-8a1e-40fd-ca88-115da218d6db"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\"\"\"\n",
        "nlp = spacy.load(\"en_pipeline\")\n",
        "doc = nlp(text)\n",
        "spacy.displacy.render(doc, style=\"ent\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
